---
title: CodeT5
author: coder
date: 2021-09-02 00:00:00 +0800
categories: [EMNLP]
tags: [models]
math: true
pin: false
---

- ğŸ“™Paper: [CodeT5 Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation](https://arxiv.org/pdf/2109.00859.pdf)
- ğŸ“šPublisher: `EMNLP`
- ğŸ Author Affiliation: `Salesforce Research Asia`
- ğŸ”‘Public: âœ…
- ğŸŒArchitecture
  + [x] Encoder-Decoder
  + [ ] Decoder-Only
- ğŸ“Model Size
  + `60M`; `220M`; `770M`
- ğŸ—‚ï¸Data pre-processing
  + Data Resource
    * CodeSearchNet
    * BigQuery
  + De-duplication: âŒ
  + Filter Strategies
    * /
- ğŸ‰Tokenizer
  + Technology
    * [x] Byte-level Byte-Pair-Encoding (BBPE)
    * [ ] SentencePiece
  + Details
    * we train a byte-level BPE tokenizer. We allow tokens to extend across whitespace (excluding newline characters) so that common code idioms (e.g., import numpy as np) are represented as single tokens in the vocabulary. 
- ğŸ§ªHyperparameters (CodeT5 770M)
  + optimizer: AdamW
    * betas: /
    * eps: /
  + batch size: /
  + context window: `2,048`
  + gradient accumulation steps: /
  + warmup steps: `1,000`
  + learning rate: `2e-4`
  + weight decay: `0.05`
  + decay schedule
    * [ ] Cosine
    * [x] Linear
    * [ ] Polynomial
    * [ ] Inverse Square
  + precision floating point: `fp16`
- ğŸƒâ€â™€ï¸Training
  + model initialization: from scratch
  + training strategies
    * [x] left-to-right
    * [ ] fill-in-the-middle
  + trained tokens/steps: /
  + hardware: 16 A100 GPUs with 40G memory
  + training time: 21 days
