---
title: GPT-CC
author: coder
date: 2021-10-01 00:00:00 +0800
categories: [other]
tags: [models]
math: true
pin: false
---

- ðŸ“™Paper: [GPT Code Clippy The Open Source version of GitHub Copilot](https://github.com/CodedotAl/gpt-code-clippy/wiki)
- ðŸ“šPublisher: `other`
- ðŸ Author Affiliation: `CodedotAI`
- ðŸ”‘Public: âœ…
- ðŸŒArchitecture
  + [ ] Encoder-Decoder
  + [x] Decoder-Only
- ðŸ“Model Size
  + `125M`; `1.3B`
- ðŸ—‚ï¸Data pre-processing
  + Data Resource
    * The dataset used to train GPT-CC is obtained from SEART GitHub Search
    * The GitHub repositories contain in the Pile
  + De-duplication: âœ…
  + Filter Strategies
    * \>10 GitHub stars
    * \>2 commits
    * must have a licence
    * exclude forks
    * size \< 70708 bytes
- ðŸ‰Tokenizer
  + Technology
    * [x] Byte-level Byte-Pair-Encoding (BBPE)
    * [ ] SentencePiece
  + Details
    * GPT-Neo tokenizer
- ðŸ§ªHyperparameters (GPT-CC 1.3B)
  + optimizer: AdaFa
    * betas: /
    * eps: /
  + batch size: `24`
  + context window: `1,024`
  + gradient accumulation steps: /
  + warmup steps: `5,000`
  + learning rate: `2e-5`
  + weight decay: `0.1`
  + decay schedule
    * [ ] Cosine
    * [x] Linear
    * [ ] Polynomial
    * [ ] Inverse Square
  + precision floating point: /
- ðŸƒâ€â™€ï¸Training
  + model initialization: GPT-Neo
  + training strategies
    * [x] left-to-right
    * [ ] fill-in-the-middle
  + trained tokens/steps: /
  + hardware: /
  + training time: /
