---
title: CodeRL
author: coder
date: 2022-06-05 00:00:00 +0800
categories: [NeurIPS]
tags: [models]
math: true
pin: false
---

- ğŸ“™Paper: [CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning](https://arxiv.org/pdf/2207.01780.pdf)
- ğŸ“šPublisher: `NeurIPS`
- ğŸ Author Affiliation: `Salesforce Research`
- ğŸ”‘Public: âœ…
- ğŸŒArchitecture
  + [x] Encoder-Decoder
  + [ ] Decoder-Only
- ğŸ“Model Size
  + `770M`
- ğŸ—‚ï¸Data pre-processing
  + Data Resource
    * APPS
  + De-duplication: /
  + Filter Strategies
    * /
- ğŸ‰Tokenizer
  + Technology
    * [x] Byte-level Byte-Pair-Encoding (BBPE)
    * [ ] SentencePiece
  + Details
    * We adopt the code-specific tokenizer from Wang et al. [2021].
- ğŸ§ªHyperparameters (CodeRL 770M)
  + optimizer: AdamW
    * betas: /
    * eps: /
  + batch size: `64`
  + context window: /
  + gradient accumulation steps: /
  + warmup steps: /
  + learning rate: /
  + weight decay: /
  + decay schedule
    * [ ] Cosine
    * [ ] Linear
    * [x] Polynomial
    * [ ] Inverse Square
  + precision floating point: /
- ğŸƒâ€â™€ï¸Training
  + model initialization: CodeT5-large-ntp-py
  + training strategies
    * [x] left-to-right
    * [ ] fill-in-the-middle
    * [x] reinforcement learning
  + trained tokens/steps: /
  + hardware: 1 A100 GPU
  + training time: fine-tuned CodeT5-large 30 hours
