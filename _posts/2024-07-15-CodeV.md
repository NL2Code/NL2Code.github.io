---
title: CodeV
author: coder
date: 2024-07-15 00:00:00 +0800
categories: [arxiv]
tags: [models]
math: true
---

- üìôPaper: [CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization](https://arxiv.org/abs/2407.10424)
- üìöPublisher: `arxiv`
- üè†Author Affiliation: `ICT, CAS`, `University of Science and Technology of China`, `ByteDance Ltd.`, `Cambricon Technologies`
- GitHub: [https://iprc-dip.github.io/CodeV](https://iprc-dip.github.io/CodeV)
- Models: [CodeV](https://huggingface.co/collections/yang-z/codev-6698a560cd94e61a9675fa2a)
- Contributions:
  + Novel Dataset Construction: We propose an effective description-code dataset construction approach by providing GPT-3.5 with Verilog code to summarize corresponding descriptions in a multi-level manner. This method outperforms prior data construction work on Verilog generation tasks.
  + SOTA Verilog Generation Models: Based on this method, we present a series of SOTA Verilog generation LLMs, namely CodeV. Among them, CodeV-CodeQwen achieves 77.6% pass@1 on the VerilogEval-machine benchmark and 53.2% on the VerilogEval-human benchmark, outperforming GPT-4 and previous SOTA model BetterV, and also achieves a 93.1% syntax pass rate and 55.2% function pass rate on the RTLLM benchmark, outperforming previous SOTA model RTLCoder.
  + Open Source Contribution: We plan to open-source a series of CodeV models, to support the advancement and collaboration within the LLM, electronic design automation (EDA), and programming language communities.
