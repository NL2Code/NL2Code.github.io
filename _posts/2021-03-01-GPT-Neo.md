---
title: GPT-Neo
author: coder
date: 2021-03-01 00:00:00 +0800
categories: [other]
tags: [models]
math: true
pin: false
---

- 📙Paper: [GPT-Neo](https://github.com/EleutherAI/gpt-neo)
- 📚Publisher: `other`
- 🏠Author Affiliation: `EleutherAI`
- 🔑Public: ✅
- 🌐Architecture
  + [ ] Encoder-Decoder
  + [x] Decoder-Only
- 📏Model Size
  + `125M`; `1.3B`; `2.7B`
- 🗂️Data pre-processing
  + Data Resource
    * The Pile
  + De-duplication: /
  + Filter Strategies
    * /
- 🍉Tokenizer
  + Technology
    * [x] Byte-level Byte-Pair-Encoding (BBPE)
    * [ ] SentencePiece
  + Details
    * GPT-2 tokenizer
    * Train a new tokenizer on your own dataset
- 🧪Hyperparameters (GPT-Neo 2.7B)
  + optimizer: Adam
    * betas: 0.9, 0.95
    * eps: 1e-8
  + batch size: /
  + context window: `2,048`
  + gradient accumulation steps: /
  + warmup steps: `3,000`
  + learning rate: /
  + weight decay: `0.1`
  + decay schedule
    * [x] Cosine
    * [ ] Linear
    * [ ] Polynomial
    * [ ] Inverse Square
  + precision floating point: /
- 🏃‍♀️Training
  + model initialization: from scratch
  + training strategies
    * [x] left-to-right
    * [ ] fill-in-the-middle
  + trained tokens/steps: /
  + hardware: Training and inference is officially supported on TPU and should work on GPU as well.
  + training time: /
