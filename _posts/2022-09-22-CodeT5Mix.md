---
title: CodeT5Mix
author: coder
date: 2022-09-22 00:00:00 +0800
categories: [arxiv]
tags: [models]
math: true
pin: false
---

- 📙Paper: [CodeT5Mix A Pretrained Mixture of Encoder-decoder Transformers for Code Understanding and Generation](https://openreview.net/pdf?id=VPCi3STZcaO)
- 📚Publisher: `arxiv`
- 🏠Author Affiliation: `Anonymous`
- 🔑Public: ✅ (promise)
- 🌐Architecture
  + [x] Encoder-Decoder
  + [ ] Decoder-Only
- 📏Model Size
  + `220M`; `770M`
- 🗂️Data pre-processing
  + Data Resource
    * CodeSearchNet
    * CodeParrot
  + De-duplication: ✅
  + Filter Strategies
    * filter the dataset by preserving only permissively licensed code
    * files with 50 to 2000 tokens
    * besides, we filter out the overlapped subset with CodeSearchNet and other downstream tasks by checking their GitHub repositories
- 🍉Tokenizer
  + Technology
    * [x] Byte-level Byte-Pair-Encoding (BBPE)
    * [ ] SentencePiece
  + Details
    * CodeT5 tokenizer
- 🧪Hyperparameters (CodeT5Mix 770M)
  + optimizer: AdamW
    * eps: /
    * bs: /
  + batch size: /
  + context window: /
  + gradient accumulation steps: /
  + warmup steps: /
  + learning rate: /
  + weight decay: `0.1`
  + decay schedule
    * [ ] Cosine
    * [x] Linear
    * [ ] Polynomial
    * [ ] Inverse Square
  + precision floating point: `fp16`
- 🏃‍♀️Training
  + model initialization: from scratch
  + training strategies
    * [x] left-to-right
    * [ ] fill-in-the-middle
  + trained tokens/steps: /
  + hardware: 16 A100 GPUs with 40G memory
  + training time: /
