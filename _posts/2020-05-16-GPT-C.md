---
title: GPT-C
author: coder
date: 2020-05-16 00:00:00 +0800
categories: [FSE]
tags: [models]
math: true
pin: false
---

- ğŸ“™Paper: [IntelliCode compose code generation using transformer](https://dl.acm.org/doi/abs/10.1145/3368089.3417058)
- ğŸ“šPublisher: `FSE`
- ğŸ Author Affiliation: `Microsoft`
- ğŸ”‘Public: âŒ
- ğŸŒArchitecture
  + [ ] Encoder-Decoder
  + [x] Decoder-Only
- ğŸ“Model Size
  + `366M`
- ğŸ—‚ï¸Data pre-processing
  + Data Resource
    * We collect a large unsupervised source code dataset to train and evaluate the code sequence completion model. It comprises over 1.2 billion lines of source code in Python, C#, JavaScript and TypeScript programming languages. A total of over 52000 top-starred (non-fork) projects in GitHub has been selected, containing libraries from a diverse set of domains, with over 4.7 million source code files.
  + De-duplication: âŒ
  + Filter Strategies
    * /
- ğŸ‰Tokenizer
  + Technology
    * [x] Byte-level Byte-Pair-Encoding (BBPE)
    * [ ] SentencePiece
  + Details
    * /
- ğŸ§ªHyperparameters (GPT-C 366M)
  + optimizer: Adam
    * betas: /
    * eps: /
  + batch size: /
  + context window: `1,024`
  + gradient accumulation steps: /
  + warmup steps: /
  + learning rate: `6.25e-5`
  + weight decay: /
  + decay schedule
    * [x] Cosine
    * [ ] Linear
    * [ ] Polynomial
    * [ ] Inverse Square
  + precision floating point: /
- ğŸƒâ€â™€ï¸Training
  + model initialization: from scratch
  + training strategies
    * [x] left-to-right
    * [ ] fill-in-the-middle
  + trained tokens/steps: /
  + hardware: 5 Lambda V100 boxes, each having sixteen V100 GPUs with 32 GB HBM2 memory
  + training time: /
