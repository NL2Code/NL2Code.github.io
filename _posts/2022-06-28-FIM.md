---
title: FIM
author: coder
date: 2022-06-28 00:00:00 +0800
categories: [arxiv]
tags: [models]
math: true
pin: false
---

- ğŸ“™Paper: [Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/pdf/2207.14255.pdf)
- ğŸ“šPublisher: `arxiv`
- ğŸ Author Affiliation: `OpenAI`
- ğŸ”‘Public: âŒ
- ğŸŒArchitecture
  + [ ] Encoder-Decoder
  + [x] Decoder-Only
- ğŸ“Model Size
  + `50M`; `77M`; `164M`; `411M`; `844M`; `1.4B`; `2.8B`; `6.9B`
- ğŸ—‚ï¸Data pre-processing
  + Data Resource
    * Same with Codex: which is a 159 GB Python dataset scraped in May 2020. 
  + De-duplication: âœ…
  + Filter Strategies
    * We filtered out files which were likely auto-generated: average line length greater than 100;
    * maximum line length greater than 1000;
    * contain a small percentage of alphaunmeric characters.
- ğŸ‰Tokenizer
  + Technology
    * [x] Byte-level Byte-Pair-Encoding (BBPE)
    * [ ] SentencePiece
  + Details
    * Same with Codex: GPT3 tokenizer+additional set of tokens for representing whitespace runs of different lengths
- ğŸ§ªHyperparameters (FIM 6.9B)
  + optimizer: Adam
    * betas: /
    * eps: /
  + batch size: `2M`
  + context window: `2,048`
  + gradient accumulation steps: /
  + warmup steps: /
  + learning rate: `2.4e-4`
  + weight decay: /
  + decay schedule
    * [ ] Cosine
    * [ ] Linear
    * [ ] Polynomial
    * [ ] Inverse Square
  + precision floating point: /
- ğŸƒâ€â™€ï¸Training
  + model initialization: from scratch
  + training strategies
    * [ ] left-to-right
    * [x] fill-in-the-middle
  + trained tokens/steps: 100B tokens
  + hardware: /
  + training time: /
