---
title: InverseCoder
author: coder
date: 2024-07-09 00:00:00 +0800
categories: [arxiv]
tags: [models]
math: true
---

- üìôPaper: [InverseCoder: Unleashing the Power of Instruction-Tuned Code LLMs with Inverse-Instruct](https://arxiv.org/abs/2407.05700)
- üìöPublisher: `arxiv`
- üè†Author Affiliation: `ICT, CAS`, `Baidu`, `Autodesk Research`
- Contributions:
  - We introduce INVERSE-INSTRUCT, a simple yet effective instruction tuning approach exploiting the mismatch of code-generation and instruction-generation. 
  - We make thorough analysis on INVERSE-INSTRUCT, including the component of generated dataset,
    the impact of data size, etc. We find that the self-consistency between the code generation and
    summarization is predictive of the effectiveness of INVERSE-INSTRUCT prior to training.
  - Based on INVERSE-INSTRUCT, we present a series of code LLMs named InverseCoder, which
    achieves SOTA or comparative results on a wide range of benchmarks including Python code
    generation, multilingual code completion, and data science problems.
- Models and Datasets: [InverseCoder](https://huggingface.co/collections/wyt2000/inversecoder-668cfb4777108dfc2e93c555)

