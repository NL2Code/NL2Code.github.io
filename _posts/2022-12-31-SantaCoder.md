---
title: SantaCoder
author: coder
date: 2022-12-31 00:00:00 +0800
categories: [arxiv]
tags: [models]
math: true
pin: false
---

- ğŸ“™Paper: [SantaCoder don't reach for the stars!](https://arxiv.org/pdf/2301.03988.pdf)
- ğŸ“šPublisher: `arxiv`
- ğŸ Author Affiliation: `huggingface`
- ğŸ”‘Public: âœ…
- ğŸŒArchitecture
  + [ ] Encoder-Decoder
  + [x] Decoder-Only
- ğŸ“Model Size
  + `1.1B`
- ğŸ—‚ï¸Data pre-processing
  + Data Resource
    * The Stack v1.1
  + De-duplication: âœ…
  + Filter Strategies
    * removing data from opt-out requests, near-deduplication, PII-redaction
    * filtering based on line-length and percentage of alphanumeric characters
    * removing files that contained test-samples from the following benchmarks: HumanEval, APPS, MBPP, MultiPL-E
- ğŸ‰Tokenizer
  + Technology
    * [x] Byte-level Byte-Pair-Encoding (BBPE)
    * [ ] SentencePiece
  + Details
    * We train a HuggingFace Tokenizer on 600k rows (around 2.6GB) of data.
- ğŸ§ªHyperparameters (SantaCoder 1.1B)
  + optimizer: Adam
    * betas: 0.9, 0.95
    * eps: 1e-8
  + global batch-size: `192`
  + context window: /
  + gradient accumulation steps: /
  + warmup steps: /
  + learning rate: `2e-4`
  + weight decay: `0.1`
  + decay schedule
    * [x] Cosine
    * [ ] Linear
    * [ ] Polynomial
    * [ ] Inverse Square
  + precision floating point: `fp16`
- ğŸƒâ€â™€ï¸Training
  + model initialization: from scratch
  + training strategies
    * [ ] left-to-right
    * [x] fill-in-the-middle
  + trained tokens/steps: 118B tokens
  + hardware: 96 Tesla V100 GPUs
  + training time: Each training run takes 3.1 days
