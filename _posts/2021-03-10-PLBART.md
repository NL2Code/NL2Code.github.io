---
title: PLBART
author: coder
date: 2021-03-10 00:00:00 +0800
categories: [NAACL]
tags: [models]
math: true
pin: false
---

- ğŸ“™Paper: [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/pdf/2103.06333.pdf)
- ğŸ“šPublisher: `NAACL`
- ğŸ Author Affiliation: `University of California`
- ğŸ”‘Public: âœ…
- ğŸŒArchitecture
  + [x] Encoder-Decoder
  + [ ] Decoder-Only
- ğŸ“Model Size
  + `140M`; `406M`
- ğŸ—‚ï¸Data pre-processing
  + Data Resource
    * We download all the GitHub repositories associated with Java and Python languages available on Google BigQuery
    * We collect the StackOverflow posts by downloading the data dump from stackexchange
  + De-duplication: âŒ
  + Filter Strategies
    * /
- ğŸ‰Tokenizer
  + Technology
    * [ ] Byte-level Byte-Pair-Encoding (BBPE)
    * [x] SentencePiece
  + Details
    * /
- ğŸ§ªHyperparameters (PLBART 406M)
  + optimizer: Adam
    * betas: âˆ’, 0.98
    * eps: 1e-6
  + batch size: /
  + context window: `768`
  + gradient accumulation steps: /
  + warmup steps: /
  + learning rate: `5e-5`
  + weight decay: /
  + decay schedule
    * [ ] Cosine
    * [x] Linear
    * [ ] Polynomial
    * [ ] Inverse Square
  + precision floating point: `fp16`
- ğŸƒâ€â™€ï¸Training
  + model initialization: /
  + training strategies
    * [x] left-to-right
    * [ ] fill-in-the-middle
  + trained tokens/steps: /
  + hardware: 8 Nvidia GeForce RTX 2080 Ti GPUs
  + training time: /
