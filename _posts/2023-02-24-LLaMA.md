---
title: LLaMA
author: coder
date: 2023-02-24 00:00:00 +0800
categories: [arxiv]
tags: [models]
math: true
pin: false
---

- ğŸ“™Paper: [LLaMA Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)
- ğŸ“šPublisher: `arxiv`
- ğŸ Author Affiliation: `Meta AI`
- ğŸ”‘Public: âˆš
- ğŸŒArchitecture
  + [ ] Encoder-Decoder
  + [x] Decoder-Only
- ğŸ“Model Size
  + `6.7B`; `13.0B`; `32.5B`; `65.2B`
- ğŸ—‚ï¸Data pre-processing
  + Data Resource
    * Google BigQuery
  + De-duplication: âˆš
  + Filter Strategies
    * We deduplicate the resulting dataset at the file level, with exact matches.
- ğŸ‰Tokenizer
  + Technology
    * [x] Byte-level Byte-Pair-Encoding (BBPE)
    * [ ] SentencePiece
  + Details
    * /
- ğŸ§ªHyperparameters (LLaMA 65.2B)
  + optimizer: AdamW
    * betas: 0.9
    * eps: 0.95
  + batch size: `4M` tokens
  + context window: /
  + gradient accumulation steps: /
  + warmup steps: 2,000
  + learning rate: 1.5e-4
  + weight decay: 0.1
  + decay schedule
    * [x] Cosine
    * [ ] Linear
    * [ ] Polynomial
    * [ ] Inverse Square
  + precision floating point: /
- ğŸƒâ€â™€ï¸Training
  + model initialization: /
  + training strategies
    * [x] left-to-right
    * [ ] fill-in-the-middle
  + trained tokens/steps: 1.4T tokens
  + hardware: 2048 A00 GPU with 80GB of RAM
  + training time: training over our dataset containing 1.4T tokens takes approximately 21 days.
